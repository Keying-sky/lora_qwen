{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "  Hidden dimension: 896\n",
      "  Number of attention heads: 14\n",
      "  Hidden dimension per head: 64\n",
      "  Number of layers: 24\n",
      "  Vocabulary size: 151936\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# load model configuration\n",
    "config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "D = config.hidden_size             # hidden dimension \n",
    "H = config.num_attention_heads     # number of heads\n",
    "d = D // H                         # hidden dimension per head\n",
    "L= config.num_hidden_layers        # number of layers\n",
    "V = config.vocab_size              # vocabulary size\n",
    "\n",
    "print(f\"Model parameters:\")\n",
    "print(f\"  Hidden dimension: {D}\")\n",
    "print(f\"  Number of attention heads: {H}\")\n",
    "print(f\"  Hidden dimension per head: {d}\")\n",
    "print(f\"  Number of layers: {L}\")\n",
    "print(f\"  Vocabulary size: {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.716343e+13\n"
     ]
    }
   ],
   "source": [
    "from lora_qwen.flops import calculate_flops\n",
    "baseline_flops = calculate_flops(499, H, D, L, B=1, inference=True, infer_length=99)\n",
    "print(f\"{baseline_flops:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LoRA 1: With default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.444766e+15\n"
     ]
    }
   ],
   "source": [
    "from lora_qwen.flops import calculate_flops\n",
    "training_flops1 = calculate_flops(512, H, D, L, B=4, nsteps=2000, inference=False)\n",
    "evalue_flops1 = 5 * calculate_flops(499, H, D, L, B=1, inference=True, infer_length=99)  # 1/5 of max steps evaluate once\n",
    "\n",
    "flops1 = training_flops1 + evalue_flops1\n",
    "print(f\"{flops1:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LoRA 2: Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.751959e+16\n"
     ]
    }
   ],
   "source": [
    "from lora_qwen.flops import calculate_flops\n",
    "\n",
    "# 2 configurations have early stopped (see hyperparameter_result.csv)\n",
    "# so that number of evaluations for lr_rank_search is 7*5 + 2*3 = 41\n",
    "flops_lr_rank = 7 * calculate_flops(320, H, D, L, B=2, nsteps=1500, inference=False)\n",
    "flops_lr_rank = 2 * calculate_flops(320, H, D, L, B=2, nsteps=900, inference=False)  \n",
    "\n",
    "search_ctx_length = []\n",
    "for i in [128, 512, 768]:\n",
    "    search_ctx_length.append(calculate_flops(i, H, D, L, B=2, nsteps=2000, inference=False))\n",
    "flops_ctx_length = sum(search_ctx_length)\n",
    "\n",
    "# 41 for lr_rank (see above) and 5*3 for three context length search\n",
    "evalue_flops2 = (41 + 15) *  calculate_flops(499, H, D, L, B=1, inference=True, infer_length=99)\n",
    "\n",
    "\n",
    "flops2 = flops_lr_rank + flops_ctx_length + evalue_flops2\n",
    "print(f\"{flops2:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LoRA 3: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.527753e+16\n"
     ]
    }
   ],
   "source": [
    "from lora_qwen.flops import calculate_flops\n",
    "training_flops3 = calculate_flops(512, H, D, L, B=4, nsteps=9700, inference=False)\n",
    "evalue_flops3 = (3+7) * calculate_flops(499, H, D, L, B=1, inference=True, infer_length=99)\n",
    "\n",
    "flops3 = training_flops3 + evalue_flops3\n",
    "print(f\"{flops3:e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.227905e+16\n"
     ]
    }
   ],
   "source": [
    "total_flops = baseline_flops + flops1 + flops2 + flops3\n",
    "print(f\"{total_flops:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Experiment         FLOPS % of Total Budget\n",
      "0       Baseline (Inference only)  3.716343e+13           0.0372%\n",
      "1     LoRA 1 (Default parameters)  9.444766e+15           9.4448%\n",
      "2  LoRA 2 (Hyperparameter search)  1.751959e+16          17.5196%\n",
      "3            LoRA 3 (Final Model)  4.527753e+16          45.2775%\n",
      "4                           Total  7.227905e+16          72.2790%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "total_budget = 1e17 \n",
    "\n",
    "# create a dictionary with experiment names and their FLOPS\n",
    "experiments = {\n",
    "    \"Baseline (Inference only)\": baseline_flops,\n",
    "    \"LoRA 1 (Default parameters)\": flops1,\n",
    "    \"LoRA 2 (Hyperparameter search)\": flops2,\n",
    "    \"LoRA 3 (Final Model)\": flops3,\n",
    "    \"Total\": total_flops\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(list(experiments.items()), columns=[\"Experiment\", \"FLOPS\"])\n",
    "\n",
    "# scientific notation\n",
    "df[\"FLOPS (Scientific)\"] = df[\"FLOPS\"].apply(lambda x: f\"{x:.6e}\")\n",
    "\n",
    "# percentage of total budget\n",
    "df[\"% of Total Budget\"] = (df[\"FLOPS\"] / total_budget) * 100\n",
    "df[\"% of Total Budget\"] = df[\"% of Total Budget\"].apply(lambda x: f\"{x:.4f}%\")\n",
    "\n",
    "df = df.drop(columns=[\"FLOPS\"])\n",
    "df = df.rename(columns={\"FLOPS (Scientific)\": \"FLOPS\"})\n",
    "\n",
    "print(df)\n",
    "\n",
    "# save\n",
    "with open(\"../results/flops_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"Total Budget: {total_budget:.0e} FLOPS\\n\")\n",
    "    \n",
    "    # Write headers with fixed width formatting\n",
    "    f.write(f\"{'Experiment':<40} {'FLOPS':<20} {'% of Total Budget':<20}\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Write each row\n",
    "    for _, row in df.iterrows():\n",
    "        f.write(f\"{row['Experiment']:<40} {row['FLOPS']:<20} {row['% of Total Budget']:<20}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
